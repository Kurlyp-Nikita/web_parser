import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import json
from urllib.parse import urljoin, urlparse
import os
import sys

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è openpyxl –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Excel
try:
    import openpyxl
    EXCEL_AVAILABLE = True
except ImportError:
    EXCEL_AVAILABLE = False
    print("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: openpyxl –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Excel –±—É–¥–µ—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ.")


class WebParser:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })

    def parse_website(self, url, selectors=None, max_pages=1, delay=1):
        """
        –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä —Å–∞–π—Ç–æ–≤

        Args:
            url (str): URL —Å–∞–π—Ç–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
            selectors (dict): CSS —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
            max_pages (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
            delay (int): –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        """
        data = []

        try:
            for page in range(1, max_pages + 1):
                print(f"–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}...")

                # –§–æ—Ä–º–∏—Ä—É–µ–º URL –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                if page == 1:
                    current_url = url
                else:
                    # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞)
                    if '?' in url:
                        current_url = f"{url}&page={page}"
                    else:
                        current_url = f"{url}?page={page}"

                # –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
                response = self.session.get(current_url)
                response.raise_for_status()

                # –ü–∞—Ä—Å–∏–º HTML
                soup = BeautifulSoup(response.content, 'html.parser')

                # –ï—Å–ª–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –Ω–µ —É–∫–∞–∑–∞–Ω—ã, –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –æ–±—â–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                if not selectors:
                    items = self._auto_detect_items(soup)
                else:
                    items = soup.select(selectors.get('items', 'div'))

                for item in items:
                    item_data = {}

                    if selectors:
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ —É–∫–∞–∑–∞–Ω–Ω—ã–º —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º
                        for key, selector in selectors.items():
                            if key != 'items':
                                element = item.select_one(selector)
                                if element:
                                    item_data[key] = element.get_text(strip=True)
                                    # –ï—Å–ª–∏ –µ—Å—Ç—å –∞—Ç—Ä–∏–±—É—Ç href, —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Å—ã–ª–∫—É
                                    if element.get('href'):
                                        item_data[f'{key}_link'] = urljoin(url, element.get('href'))
                    else:
                        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
                        item_data = self._extract_auto_data(item)

                    if item_data:
                        data.append(item_data)

                # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                if page < max_pages:
                    time.sleep(delay)

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {e}")

        return data

    def _auto_detect_items(self, soup):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        # –ò—â–µ–º –æ–±—â–∏–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã
        selectors = [
            'div[class*="item"]',
            'div[class*="product"]',
            'div[class*="card"]',
            'div[class*="post"]',
            'article',
            'li',
            '.item',
            '.product',
            '.card',
            '.post'
        ]

        for selector in selectors:
            items = soup.select(selector)
            if len(items) > 1:
                return items

        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—Å–µ div'—ã
        return soup.find_all('div')[:10]

    def _extract_auto_data(self, item):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–∞"""
        data = {}

        # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
        title = item.find(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
        if title:
            data['title'] = title.get_text(strip=True)

        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏
        links = item.find_all('a')
        if links:
            data['links'] = [link.get('href') for link in links if link.get('href')]

        # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        images = item.find_all('img')
        if images:
            data['images'] = [img.get('src') for img in images if img.get('src')]

        # –ò—â–µ–º —Ç–µ–∫—Å—Ç
        text = item.get_text(strip=True)
        if text and len(text) > 10:
            data['text'] = text[:200] + '...' if len(text) > 200 else text

        return data

    def save_to_excel(self, data, filename='parsed_data.xlsx'):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ Excel"""
        if not EXCEL_AVAILABLE:
            print("–û—à–∏–±–∫–∞: openpyxl –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –µ–≥–æ –∫–æ–º–∞–Ω–¥–æ–π: pip install openpyxl")
            return
            
        if not data:
            print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
            return

        try:
            df = pd.DataFrame(data)
            df.to_excel(filename, index=False)
            print(f"–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ Excel: {e}")
            print("–ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ CSV –∏–ª–∏ JSON —Ñ–æ—Ä–º–∞—Ç")

    def save_to_csv(self, data, filename='parsed_data.csv'):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ CSV"""
        if not data:
            print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
            return

        try:
            df = pd.DataFrame(data)
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            print(f"–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ CSV: {e}")

    def save_to_json(self, data, filename='parsed_data.json'):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ JSON"""
        if not data:
            print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
            return

        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ JSON: {e}")

    def auto_save_all(self, data, base_filename='parsed_data'):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–æ –≤—Å–µ —Ñ–æ—Ä–º–∞—Ç—ã"""
        if not data:
            print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
            return
        
        print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º {len(data)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤...")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Excel
        if EXCEL_AVAILABLE:
            self.save_to_excel(data, f"{base_filename}.xlsx")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV
        self.save_to_csv(data, f"{base_filename}.csv")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSON
        self.save_to_json(data, f"{base_filename}.json")
        
        print("‚úÖ –í—Å–µ —Ñ–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!")


def quick_parse(url, max_pages=1, delay=1):
    """
    –ë—ã—Å—Ç—Ä—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–∞ –±–µ–∑ –ª–∏—à–Ω–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
    
    Args:
        url (str): URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
        max_pages (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1)
        delay (int): –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1 —Å–µ–∫—É–Ω–¥–∞)
    """
    print(f"üöÄ –ë—ã—Å—Ç—Ä—ã–π –ø–∞—Ä—Å–∏–Ω–≥: {url}")
    print(f"üìÑ –°—Ç—Ä–∞–Ω–∏—Ü: {max_pages}, ‚è±Ô∏è –ó–∞–¥–µ—Ä–∂–∫–∞: {delay}—Å")
    
    parser = WebParser()
    data = parser.parse_website(url, max_pages=max_pages, delay=delay)
    
    if data:
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(data)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ –≤—Å–µ —Ñ–æ—Ä–º–∞—Ç—ã
        base_filename = f"parsed_{urlparse(url).netloc.replace('.', '_')}"
        parser.auto_save_all(data, base_filename)
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–µ–≤—å—é
        print(f"\nüìã –ü–†–ï–í–¨–Æ (–ø–µ—Ä–≤—ã–µ 3 —ç–ª–µ–º–µ–Ω—Ç–∞):")
        print("-" * 50)
        for i, item in enumerate(data[:3], 1):
            print(f"–≠–ª–µ–º–µ–Ω—Ç {i}:")
            for key, value in item.items():
                if isinstance(value, str) and len(value) > 100:
                    value = value[:100] + "..."
                print(f"  {key}: {value}")
            print()
    else:
        print("‚ùå –î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")


if __name__ == "__main__":
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
    if len(sys.argv) > 1:
        url = sys.argv[1]
        max_pages = int(sys.argv[2]) if len(sys.argv) > 2 else 1
        delay = float(sys.argv[3]) if len(sys.argv) > 3 else 1
        quick_parse(url, max_pages, delay)
    else:
        # –ï—Å–ª–∏ –Ω–µ—Ç –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤, –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º URL
        url = input("–í–≤–µ–¥–∏—Ç–µ URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞: ").strip()
        if url:
            quick_parse(url)
        else:
            print("‚ùå URL –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º!")